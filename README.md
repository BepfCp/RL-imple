# RL-imple

对于一些经典的强化学习算法的实现，目前已经实现：

+ [动态规划](https://github.com/BepfCp/RL-imple/tree/master/src/DP)，包括：策略评估、策略改进、策略迭代、截断策略迭代以及价值迭代
+ [蒙特卡洛方法](https://github.com/BepfCp/RL-imple/tree/master/src/MC)，包括：MC-预测和Constant-$\alpha$ MC-Control
+ [时序差分学习](https://github.com/BepfCp/RL-imple/tree/master/src/TD)，包括：Sarsa，Q-learning，Expected Sarsa以及自定义gym环境
+ [Tile-Coding](https://github.com/BepfCp/RL-imple/tree/master/src/Tile-Coding)，包括：Tile-Coding+Q-learning
+ DQN

### 参考文献

+ Sutton R S, Barto A G. Reinforcement learning: An introduction[M]. MIT press, 2018.
+ Mnih V, Kavukcuoglu K, Silver D, et al. Human-level control through deep reinforcement learning[J]. Nature, 2015, 518(7540): 529-533.

### 致谢

+ 感谢Sutton & Barto的书《Reinforcement Leanring：An Introduction》，很经典翔实的一本强化学习入门书。
+ 感谢udacity提供的[作业](https://github.com/udacity/deep-reinforcement-learning)，环境已经帮我们搭好了，让我们能集中精力在算法实现上。
+ 感谢@[qqiang00](https://github.com/qqiang00)整理的David Silver开设的强化学习课程[UCL Course on RL](https://www.davidsilver.uk/teaching/)笔记，[实践部分](https://github.com/qqiang00/ReinforcemengLearningPractice)很值得动手练习一下。
